{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Action Classification v2.0 - Quick Start\n",
    "\n",
    "This notebook demonstrates the basic usage of the modernized action classification system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hac import ActionPredictor\n",
    "from hac.models.classifier import create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Inference\n",
    "\n",
    "Let's start with the simplest use case - predicting actions from an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize predictor\n",
    "predictor = ActionPredictor(\n",
    "    model_path=None,  # Using pretrained backbone\n",
    "    device='cuda',\n",
    "    use_pose_estimation=True\n",
    ")\n",
    "\n",
    "print(\"âœ“ Predictor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and predict\n",
    "image_path = 'path/to/your/image.jpg'\n",
    "\n",
    "result = predictor.predict_image(\n",
    "    image_path,\n",
    "    return_pose=True,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nPose: {result['pose']['class']}\")\n",
    "print(f\"\\nTop 5 Actions:\")\n",
    "for i, pred in enumerate(result['action']['predictions'], 1):\n",
    "    print(f\"  {i}. {pred['class']}: {pred['confidence']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pose detection\n",
    "if 'pose_image' in result:\n",
    "    pose_img = cv2.cvtColor(result['pose_image'], cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(pose_img)\n",
    "    plt.title(f\"Detected Pose: {result['pose']['class']}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Comparison\n",
    "\n",
    "Compare different model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Models to test\n",
    "model_names = [\n",
    "    'mobilenetv3_small_100',\n",
    "    'mobilenetv3_large_100',\n",
    "    'efficientnet_b0',\n",
    "    'resnet18',\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nTesting {model_name}...\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(\n",
    "        model_type='action',\n",
    "        model_name=model_name,\n",
    "        num_classes=40\n",
    "    )\n",
    "    model = model.cuda().eval()\n",
    "    \n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Measure inference time\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).cuda()\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    # Time\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = model(dummy_input)\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = (time.time() - start) / 100\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'params': num_params / 1e6,  # Millions\n",
    "        'latency': elapsed * 1000  # Milliseconds\n",
    "    }\n",
    "    \n",
    "    print(f\"  Params: {results[model_name]['params']:.2f}M\")\n",
    "    print(f\"  Latency: {results[model_name]['latency']:.2f}ms\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "models = list(results.keys())\n",
    "params = [results[m]['params'] for m in models]\n",
    "latencies = [results[m]['latency'] for m in models]\n",
    "\n",
    "ax1.bar(models, params)\n",
    "ax1.set_ylabel('Parameters (Millions)')\n",
    "ax1.set_title('Model Size')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "ax2.bar(models, latencies)\n",
    "ax2.set_ylabel('Latency (ms)')\n",
    "ax2.set_title('Inference Speed')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Video Processing\n",
    "\n",
    "Process a video and aggregate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'path/to/video.mp4'\n",
    "\n",
    "result = predictor.predict_video(\n",
    "    video_path,\n",
    "    sample_rate=5,  # Sample every 5 frames\n",
    "    aggregate_method='voting'\n",
    ")\n",
    "\n",
    "print(f\"Video: {result['video_path']}\")\n",
    "print(f\"Total frames: {result['total_frames']}\")\n",
    "print(f\"Sampled: {result['sampled_frames']}\")\n",
    "print(f\"\\nPredicted action: {result['prediction']}\")\n",
    "print(f\"Confidence: {result['confidence']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pose Keypoint Analysis\n",
    "\n",
    "Extract and visualize pose keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hac.inference.pose_extractor import PoseExtractor\n",
    "\n",
    "# Initialize pose extractor\n",
    "pose_extractor = PoseExtractor(\n",
    "    static_image_mode=True,\n",
    "    model_complexity=1\n",
    ")\n",
    "\n",
    "# Extract keypoints\n",
    "image = cv2.imread(image_path)\n",
    "keypoints = pose_extractor.extract_keypoints(image)\n",
    "\n",
    "if keypoints is not None:\n",
    "    print(f\"Detected {len(keypoints)} keypoints\")\n",
    "    print(f\"Keypoint format: [x, y, visibility]\")\n",
    "    print(f\"\\nFirst 5 keypoints:\")\n",
    "    print(keypoints[:5])\n",
    "    \n",
    "    # Get normalized keypoints (scale/translation invariant)\n",
    "    normalized = pose_extractor.extract_normalized_keypoints(image)\n",
    "    print(f\"\\nNormalized keypoints shape: {normalized.shape}\")\n",
    "else:\n",
    "    print(\"No pose detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom Model Training (Setup)\n",
    "\n",
    "Example of how to set up training on your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hac.data.dataset import ActionDataset\n",
    "from hac.utils.transforms import get_training_transforms, get_inference_transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming you have organized data in:\n",
    "# data/train/class1/, data/train/class2/, ...\n",
    "# data/val/class1/, data/val/class2/, ...\n",
    "\n",
    "data_root = 'path/to/your/data'\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ActionDataset(\n",
    "    root_dir=data_root,\n",
    "    split='train',\n",
    "    transform=get_training_transforms()\n",
    ")\n",
    "\n",
    "val_dataset = ActionDataset(\n",
    "    root_dir=data_root,\n",
    "    split='val',\n",
    "    transform=get_inference_transforms()\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Classes: {train_dataset.class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Inference\n",
    "\n",
    "Process multiple images efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Get all images in directory\n",
    "image_dir = 'path/to/images/'\n",
    "image_paths = glob.glob(f\"{image_dir}/*.jpg\")\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for img_path in image_paths[:10]:  # First 10 images\n",
    "    result = predictor.predict_image(img_path, return_pose=False)\n",
    "    results_list.append({\n",
    "        'image': img_path,\n",
    "        'action': result['action']['top_class'],\n",
    "        'confidence': result['action']['top_confidence']\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results_list)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Train on your data**: Use `python -m hac.training.train`\n",
    "2. **Export to ONNX**: For production deployment\n",
    "3. **Optimize for edge**: Test on Jetson/mobile\n",
    "4. **Integrate with AVs**: Add pedestrian prediction logic\n",
    "\n",
    "See the README for more details!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
